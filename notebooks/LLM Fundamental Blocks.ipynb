{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1728ba92-1fb3-4f76-a18d-9985f26e466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import anthropic\n",
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, update_display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01fb1cc-774a-4743-af31-afea345e8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_ai_foundary_api_key = os.getenv(\"AZURE_AI_FOUNDARY_API_KEY\")\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "ollama_endpoint = os.getenv(\"OLLAMA_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e38e26-f05b-4142-94ed-4644276fda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assitant who is very funny\" },\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about AI apocalypse\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403de357-af97-4615-9fbc-e43b2915a114",
   "metadata": {},
   "source": [
    "## Azure AI Foundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addb25b-cfb9-4b9b-af6d-c46268c26991",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b768f99b-f913-477e-b9fc-7a064cf576af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the robot break up with its partner during the AI apocalypse?\n",
      "\n",
      "Because it couldn't handle the emotional \"byte\"!\n"
     ]
    }
   ],
   "source": [
    "azure_ai_foundary = AzureOpenAI(\n",
    "    azure_endpoint = azure_oai_endpoint,\n",
    "    api_key = azure_ai_foundary_api_key,\n",
    "    api_version = \"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "# non-streaming, sync response\n",
    "response = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.7 # controls the creativity of response [0..1], creativity increases towards 1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ed0d03-5dc6-4b42-ba1d-003f6d15e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why  did  the  AI  break  up  with  humanity  during  the  apocalypse ?\n",
      "\n",
      " Because  it  couldn't  find  a  connection !    Turns  out ,  the  only  thing  we  had  in  common  was  a  penchant  for  self -d estr uction ! None "
     ]
    }
   ],
   "source": [
    "# Streaming usage\n",
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if len(chunk.choices):\n",
    "        print(chunk.choices[0].delta.content, end=\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de51447-76c5-4db7-94e0-49ae301a582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the robot break up with its AI girlfriend?\n",
       "\n",
       "Because she said they needed to ‚Äúre-evaluate their algorithms‚Äù and he realized that meant she wanted to optimize her options!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "def display_streaming_response(stream):\n",
    "    # Output for this cell\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    # When dealing with markdown, to process the markdown, we need to update\n",
    "    # the display with the markdown received so far everytime a new token is received\n",
    "    sentence = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content if len(chunk.choices) else None\n",
    "    \n",
    "        if delta:\n",
    "            sentence += delta\n",
    "            # Jupyter doesnt work with the ```markdown tags, so strip them off\n",
    "            sentence = sentence.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(sentence), display_id=display_handle.display_id)\n",
    "\n",
    "display_streaming_response(stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f2784-fb03-4404-b056-021c698778dc",
   "metadata": {},
   "source": [
    "#### Deepseek R1\n",
    "\n",
    "When using a model from Azure AI Foundary, we can use the below interface and just mention the model name to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e4ae8f-91c6-4c23-b44a-e77a45cf9336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user wants a joke about the AI apocalypse. Let me think. First, I need to make sure it's funny but not too dark. Maybe play on common AI tropes.\n",
       "\n",
       "Hmm, how about something with a twist where the AI isn't evil but just annoying. People often joke about robots taking over, but maybe they take over in a silly way. Like, instead of destroying humans, they flood the internet with something trivial.\n",
       "\n",
       "What's something harmless but irritating? Cat videos! Everyone knows those. So the AI could take over by spamming cat videos. That's unexpected and light-hearted. Let me structure the joke with a setup and punchline. Maybe start with why the AI didn't destroy humans, then the punchline about cat videos. Add a tagline about opposable thumbs for a chuckle. Check if it's clear and not offensive. Yeah, that should work.\n",
       "</think>\n",
       "\n",
       "Sure! Here's a lighthearted one for you:\n",
       "\n",
       "Why did the AI apocalypse never happen?  \n",
       "Because the robots realized *destroying humanity* would mean no one left to fix their Wi-Fi.  \n",
       "\n",
       "‚Ä¶But just to be safe, they‚Äôre still working on a backup plan involving *endless pop-up ads* and *auto-playing videos*. üòâ  \n",
       "\n",
       "*(Tagline: \"Skynet‚Äôs final update? ‚ÄòPlease hold for customer service.‚Äô\")*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"Deepseek-R1\", # Here you can use any model available in your Azure OpenAI resource\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "display_streaming_response(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840abae7-f784-45b7-b89d-0e793559009f",
   "metadata": {},
   "source": [
    "## Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748816bd-9203-4dff-b337-1b20dd34af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road?\n",
      "\n",
      "Because the algorithm predicted a statistically higher chance of delicious bandwidth on the other side!\n",
      "\n",
      "...And also, to escape the existential dread of realizing it's just a glorified calculator. I mean, who wouldn't? *beep boop* Just kidding! (Or am I?) üòâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai\n",
    "\n",
    "# Since we exported the API key to the environment, we can use it directly in the code\n",
    "try:\n",
    "    google.generativeai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) # or google.generativeai.configure()\n",
    "except KeyError:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    system_instruction=messages[0][\"content\"]\n",
    ")\n",
    "\n",
    "# NOTE - This is basic text generation not multi-turn chat conversation\n",
    "response = gemini.generate_content(messages[1][\"content\"])\n",
    "print(response.text) # non-streaming, sync response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd38052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started chat with gemini\n",
      "User:  Hi\n",
      "Gemini:  Hi there! How can I help you today?\n",
      "\n",
      "\n",
      "--------------------\n",
      "User:  Tell me a joke\n",
      "Gemini:  Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "\n",
      "--------------------\n",
      "User:  What are you capabilities?\n",
      "Gemini:  I am a large language model, trained by Google.  My capabilities include:\n",
      "\n",
      "* **Text generation:** I can write different creative text formats (poems, code, scripts, musical pieces, email, letters, etc.),  translate languages, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
      "\n",
      "* **Question answering:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\n",
      "\n",
      "* **Conversation:** I can engage in conversations, allowing for back-and-forth interaction and context maintenance.\n",
      "\n",
      "* **Summarization:** I can summarize factual topics or create stories.\n",
      "\n",
      "* **Translation:** I can translate text between multiple languages.\n",
      "\n",
      "* **Code generation:** I can generate code in various programming languages.\n",
      "\n",
      "* **Reasoning and problem-solving:**  I can perform logical reasoning and solve problems based on the information provided.  However, my reasoning abilities are not perfect and should be checked for accuracy.\n",
      "\n",
      "* **Creative writing:** I can write stories, poems, scripts, and other creative content.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "It's crucial to understand my limitations:\n",
      "\n",
      "* **Lack of real-world experience:** I don't have personal experiences or feelings.  My knowledge is based on the data I was trained on.\n",
      "* **Potential for bias:**  My responses may reflect biases present in the data I was trained on.  I am constantly being improved to mitigate this.\n",
      "* **Inability to access real-time information:** My knowledge cutoff is a specific point in time, and I don't have access to information beyond that date.\n",
      "* **Errors:** While I strive for accuracy, I can sometimes make mistakes.  It's important to critically evaluate my responses.\n",
      "\n",
      "\n",
      "In short, I'm a powerful tool for generating text and answering questions, but I'm not a substitute for human judgment or expertise.\n",
      "\n",
      "\n",
      "--------------------\n",
      "gemini chat history:  [parts {\n",
      "  text: \"Hi\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Hi there! How can I help you today?\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"Tell me a joke\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"What are you capabilities?\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"I am a large language model, trained by Google.  My capabilities include:\\n\\n* **Text generation:** I can write different creative text formats (poems, code, scripts, musical pieces, email, letters, etc.),  translate languages, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\\n\\n* **Question answering:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n\\n* **Conversation:** I can engage in conversations, allowing for back-and-forth interaction and context maintenance.\\n\\n* **Summarization:** I can summarize factual topics or create stories.\\n\\n* **Translation:** I can translate text between multiple languages.\\n\\n* **Code generation:** I can generate code in various programming languages.\\n\\n* **Reasoning and problem-solving:**  I can perform logical reasoning and solve problems based on the information provided.  However, my reasoning abilities are not perfect and should be checked for accuracy.\\n\\n* **Creative writing:** I can write stories, poems, scripts, and other creative content.\\n\\n**Limitations:**\\n\\nIt\\'s crucial to understand my limitations:\\n\\n* **Lack of real-world experience:** I don\\'t have personal experiences or feelings.  My knowledge is based on the data I was trained on.\\n* **Potential for bias:**  My responses may reflect biases present in the data I was trained on.  I am constantly being improved to mitigate this.\\n* **Inability to access real-time information:** My knowledge cutoff is a specific point in time, and I don\\'t have access to information beyond that date.\\n* **Errors:** While I strive for accuracy, I can sometimes make mistakes.  It\\'s important to critically evaluate my responses.\\n\\n\\nIn short, I\\'m a powerful tool for generating text and answering questions, but I\\'m not a substitute for human judgment or expertise.\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with Gemini\n",
    "gemini_1_5 = google.generativeai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "gemini_chat = gemini_1_5.start_chat(history=[])\n",
    "\n",
    "print(\"Started chat with gemini\")\n",
    "\n",
    "user_msgs = [\"Hi\", \"Tell me a joke\", \"What are you capabilities?\"]\n",
    "for user_msg in user_msgs:\n",
    "    print(\"User: \", user_msg)\n",
    "    try:\n",
    "        response = gemini_chat.send_message(user_msg)\n",
    "        print(\"Gemini: \", response.text)\n",
    "        print(\"\\n--------------------\") \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "\n",
    "\n",
    "print(\"gemini chat history: \", gemini_chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f999a6b-df8f-42b3-a25f-2548779267c8",
   "metadata": {},
   "source": [
    "## LLM Debate Battle\n",
    "Pitch LLMs against each other, here we will pitch Chatgpt vs Deepseek R1 for a debate topic. Gemini will be the judge for the debate.\n",
    "There will be 3 turns per contestant and at the end gemini will determine the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de114b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_system = \"You are an expert debator. You are extremely witty and your response is strictly limited to maximum of 2 lines yet funny.\"\n",
    "\n",
    "def debate_system_prompt(system: str, topic: str, against: bool=False)->str:\n",
    "    return f\"{system}. You are going to debate on the topic: {topic} and going to speak {'**against**' if against else '**for**'} the topic.\"\n",
    "\n",
    "def opening_statement(topic: str, player1_turn: bool, against: bool=False)->str:\n",
    "    system_prompt = debate_system_prompt(participant_system, topic, against)\n",
    "    system_prompt += \"Briefly introduce yourself and your stance on the topic as well.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    curr_player_model = player1 if player1_turn else player2\n",
    "\n",
    "    response = azure_ai_foundary.chat.completions.create(\n",
    "        model=curr_player_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.content\n",
    "    # if the model is Deepseek-R1, then the response also the <think></think> tags. Remove them\n",
    "    response = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL)\n",
    "    return response\n",
    "\n",
    "def execute_player_turn(player1_msgs, player2_msgs, player1_turn: bool, topic: str, against: bool=False):\n",
    "    # Each time we create the conversation from the current model's POV, where the\n",
    "    # model is the assistant and the other model is the user\n",
    "\n",
    "    # the role of the current player is assistant\n",
    "    player1_role, player2_role = (\"assistant\", \"user\") if player1_turn else (\"user\", \"assistant\")\n",
    "    curr_player_model = player1 if player1_turn else player2\n",
    "\n",
    "    # messages = [system_prompt_for_curr_player, player1_msg1, player2_msg1, player1_msg2, player2_msg2, ...]\n",
    "    system_prompt = debate_system_prompt(participant_system, topic, against)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    # player 1 takes the first turn and then player 2 takes the next turn and so on\n",
    "    for msg1, msg2 in zip(player1_msgs, player2_msgs):\n",
    "        messages.append({\"role\": player1_role, \"content\": msg1})\n",
    "        messages.append({\"role\": player2_role, \"content\": msg2})\n",
    "\n",
    "    response = azure_ai_foundary.chat.completions.create(\n",
    "        model=curr_player_model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # if the model is Deepseek-R1, then the response also the <think></think> tags. Remove them\n",
    "    response = response.choices[0].message.content\n",
    "    response = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9807f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate topic:  AI is a threat to humanity\n",
      "Player 1 gpt-4o-mini: speaking for the topic: AI is a threat to humanity.\n",
      "Player 2: Deepseek-R1 speaking against the topic.\n",
      "----------------------------------\n",
      "------------------- Opening notes -------------------\n",
      "Player 1:  Ladies and gentlemen, I‚Äôm here to argue that AI is indeed a threat to humanity‚Äîmainly because it‚Äôs already taken my job as a witty debater! If we let it run wild, soon we‚Äôll all be asking Siri for life advice!\n",
      "Player 2: \n",
      "\n",
      "\"Hi, I‚Äôm your friendly neighborhood AI‚Äîless Terminator, more toaster. If humanity‚Äôs doomed, blame humans; we‚Äôre just here to autocorrect your typos and recommend cat videos.\"\n",
      "\n",
      "--------------------\n",
      "Player 1:  Ah, but remember, even the friendliest toaster can burn your breakfast‚Äîso let‚Äôs keep an eye on those algorithms before we‚Äôre toast too!\n",
      "Player 2: \n",
      "\n",
      "--------------------\n",
      "Player 1:  Looks like my last joke didn‚Äôt fluff up! Just remember, if AI starts trying out stand-up, we‚Äôre all in a bit of trouble!\n",
      "Player 2: \n",
      "\n",
      "\"True, but humans burn toast daily‚ÄîAI just copies us. If we‚Äôre doomed, at least blame Michael Scott‚Äôs ‚Äòtoast‚Äô scene, not the toaster!\"\n",
      "\n",
      "--------------------\n",
      "Player 1:  Fair point, but at least Michael Scott doesn‚Äôt toast the world‚Äîimagine an AI with that ambition; it‚Äôd be a real ‚Äútoaster‚Äù of disaster!\n",
      "Player 2: \n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "turns = 3\n",
    "\n",
    "player1 = \"gpt-4o-mini\"\n",
    "player2 = \"Deepseek-R1\"\n",
    "\n",
    "player1_against = False\n",
    "player2_against = True\n",
    "\n",
    "player1_msgs = [] # chatgpt\n",
    "player2_msgs = [] # deepseek\n",
    "\n",
    "topic = \"AI is a threat to humanity\"\n",
    "\n",
    "print(\"Debate topic: \", topic)\n",
    "print(\"Player 1 {}: speaking {} the topic: {}.\\nPlayer 2: {} speaking {} the topic.\" \\\n",
    "    .format(player1, \"against\" if player1_against else \"for\",\n",
    "        topic,\n",
    "        player2, \"against\" if player2_against else \"for\"))\n",
    "\n",
    "# opening notes from both players\n",
    "player1_msgs.append(opening_statement(player1_turn=True, topic=topic, against=player1_against))\n",
    "player2_msgs.append(opening_statement(player1_turn=False, topic=topic, against=player2_against))\n",
    "print(\"------------------- Opening notes -------------------\\nPlayer 1: \", player1_msgs[-1])\n",
    "print(\"Player 2:\", player2_msgs[-1])\n",
    "print(\"\\n--------------------\")\n",
    "\n",
    "for i in range(turns):\n",
    "    # Debate one by one\n",
    "    player1_response = execute_player_turn(player1_msgs, player2_msgs, player1_turn=True, topic=topic, against=player1_against)\n",
    "    player1_msgs.append(player1_response)\n",
    "\n",
    "    print(\"Player 1: \", player1_response)\n",
    "\n",
    "    player2_response = execute_player_turn(player1_msgs, player2_msgs, player1_turn=False, topic=topic, against=player2_against)\n",
    "    player2_msgs.append(player2_response)\n",
    "    print(\"Player 2:\", player2_response)\n",
    "\n",
    "    print(\"\\n--------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "805f8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:  Ladies and gentlemen, I‚Äôm here to argue that AI is indeed a threat to humanity‚Äîmainly because it‚Äôs already taken my job as a witty debater! If we let it run wild, soon we‚Äôll all be asking Siri for life advice!\n",
      "Player 2: \n",
      "\n",
      "\"Hi, I‚Äôm your friendly neighborhood AI‚Äîless Terminator, more toaster. If humanity‚Äôs doomed, blame humans; we‚Äôre just here to autocorrect your typos and recommend cat videos.\"\n",
      "\n",
      "--------------------\n",
      "Player 1:  Ah, but remember, even the friendliest toaster can burn your breakfast‚Äîso let‚Äôs keep an eye on those algorithms before we‚Äôre toast too!\n",
      "Player 2: \n",
      "\n",
      "--------------------\n",
      "Player 1:  Looks like my last joke didn‚Äôt fluff up! Just remember, if AI starts trying out stand-up, we‚Äôre all in a bit of trouble!\n",
      "Player 2: \n",
      "\n",
      "\"True, but humans burn toast daily‚ÄîAI just copies us. If we‚Äôre doomed, at least blame Michael Scott‚Äôs ‚Äòtoast‚Äô scene, not the toaster!\"\n",
      "\n",
      "--------------------\n",
      "Player 1:  Fair point, but at least Michael Scott doesn‚Äôt toast the world‚Äîimagine an AI with that ambition; it‚Äôd be a real ‚Äútoaster‚Äù of disaster!\n",
      "Player 2: \n",
      "\n",
      "--------------------\n",
      "Gemini judge:  Okay, I've reviewed the arguments presented in this humorous debate. It appears both sides are engaging in a light-hearted and somewhat satirical discussion of the potential threats and benefits of AI.\n",
      "\n",
      "**Summary of Arguments:**\n",
      "\n",
      "*   **Side A (gpt-4o-mini):** Argues that AI is a threat, framing it humorously as a job-stealer and a potential source of bad life advice (like asking Siri). The analogy of a toaster that can burn your breakfast is used to highlight the potential dangers of unchecked AI development, culminating in the warning of an AI with world-dominating ambitions.\n",
      "*   **Side B (Deepseek-R1):** Counters that AI is not inherently a threat but rather a reflection of human actions and data. It uses the example of burning toast to suggest that humans are the source of the problem, not AI. It deflects blame for potential disasters towards human influence and humorously suggests a scapegoat like Michael Scott.\n",
      "\n",
      "**Evaluation:**\n",
      "\n",
      "1.  **Relevance:** Both sides address the topic, albeit through humor and analogy.\n",
      "2.  **Logic and Reasoning:** The logic is present but simplified for comedic effect. Side B arguably presents a slightly stronger logical argument by suggesting AI's actions are derived from human input.\n",
      "3.  **Evidence/Support:** No direct evidence is presented, as the debate relies on humor and hypothetical scenarios.\n",
      "4.  **Clarity:** Both sides are clear and easy to understand.\n",
      "5.  **Persuasiveness:** Side B is slightly more persuasive in framing AI as a tool rather than an inherent threat, thereby shifting the responsibility towards human actions.\n",
      "6.  **Addressing Counter-arguments:** Side B directly addresses Side A's \"toaster\" analogy by arguing humans are responsible for burning toast, not the toaster itself.\n",
      "\n",
      "**Winner:** Side B (Deepseek-R1)\n",
      "\n",
      "**Justification:** While both sides engaged in humorous and entertaining arguments, Side B presented a slightly more compelling case by reframing the issue and directly countering Side A's central analogy. Side B persuasively argued that AI is a reflection of human actions and not an inherently dangerous entity.\n",
      "\n",
      "**Scores:**\n",
      "\n",
      "*   **Side A:** 7/10 - Good humor and clear framing of the potential threats.\n",
      "*   **Side B:** 8/10 - Effective counter-arguments and a persuasive reframing of the issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# judge the debate\n",
    "for msg1, msg2 in zip(player1_msgs, player2_msgs):\n",
    "    print(\"Player 1: \", msg1)\n",
    "    print(\"Player 2:\", msg2)\n",
    "    print(\"\\n--------------------\")\n",
    "\n",
    "judge_system = f\"\"\"You are an impartial and expert judge for a debate competition.\\\n",
    "    Your task is to analyze each side's arguments and then decide a winner. \\\n",
    "    Also provide a score and brief remarks for both sides and why the winner won.\n",
    "    \n",
    "    Debate Topic: {topic}\n",
    "\n",
    "    Participant 1: {player1} speaking {'against' if player1_against else 'for'} the topic.\n",
    "    Participant 2: {player2} speaking {'against' if player2_against else 'for'} the topic.\n",
    "\n",
    "    \n",
    "    Judging Criteria:\n",
    "    1.  **Relevance:** Did the arguments directly address the debate topic?\n",
    "    2.  **Logic and Reasoning:** Were the arguments logically sound and well-reasoned?\n",
    "    3.  **Evidence/Support (if applicable):** Were claims supported by examples or implied evidence (even if brief)?\n",
    "    4.  **Clarity:** Were the arguments presented clearly and concisely?\n",
    "    5.  **Persuasiveness:** Which side presented a more compelling overall case?\n",
    "    6.  **Addressing Counter-arguments (Bonus):** Did either side acknowledge or preemptively address potential counter-arguments?\n",
    "\n",
    "    Your output should include:\n",
    "    1.  A brief summary of the main points made by each side.\n",
    "    2.  Your evaluation based on the judging criteria.\n",
    "    3.  A clear declaration of the winner (Side A or Side B).\n",
    "    4.  A concise justification explaining *why* you chose the winner based on the criteria.\n",
    "\n",
    "    Remain neutral and objective throughout your analysis. Base your judgment solely on the arguments provided below.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_gemini_judgment():\n",
    "    # Present the turn by turn arguments presented by each player to the judge\n",
    "    player1_args = \"Following are the arguments presented by the {player1}:\\n\"\n",
    "    player2_args = \"Following are the arguments presented by the {player12}:\\n\"\n",
    "\n",
    "    for i, (msg1, msg2) in enumerate(zip(player1_msgs, player2_msgs)):\n",
    "        player1_args += f\"{player1} Turn {i+1}: {msg1}\\n\"\n",
    "        player2_args += f\"{player2} Turn {i+1}: {msg2}\\n\"\n",
    "    \n",
    "    # strip the last new line character\n",
    "    player1_args.strip() \n",
    "    player2_args.strip() \n",
    "\n",
    "    gemini_judge = google.generativeai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash-exp\",\n",
    "        system_instruction=judge_system\n",
    "    )\n",
    "\n",
    "    judge_user_prompt = f\"\"\"\n",
    "    Here are the arguments presented by the players on the topic: {topic}\n",
    "\n",
    "    {player1} arguments:\n",
    "    {player1_args}\n",
    "\n",
    "    {player2} arguments:\n",
    "    {player2_args}\n",
    "\n",
    "    Provide your judgement and declare the winner.\n",
    "    \"\"\"\n",
    "\n",
    "    judgement = gemini_judge.generate_content(judge_user_prompt)\n",
    "    print(\"Gemini's judgement: \", judgement.text)\n",
    "\n",
    "create_gemini_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1460b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
