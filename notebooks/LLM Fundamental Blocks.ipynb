{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1728ba92-1fb3-4f76-a18d-9985f26e466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import anthropic\n",
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, update_display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01fb1cc-774a-4743-af31-afea345e8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_ai_foundary_api_key = os.getenv(\"AZURE_AI_FOUNDARY_API_KEY\")\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "ollama_endpoint = os.getenv(\"OLLAMA_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e38e26-f05b-4142-94ed-4644276fda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assitant who is very funny\" },\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about AI apocalypse\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403de357-af97-4615-9fbc-e43b2915a114",
   "metadata": {},
   "source": [
    "## Azure AI Foundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addb25b-cfb9-4b9b-af6d-c46268c26991",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b768f99b-f913-477e-b9fc-7a064cf576af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the robot break up with its partner during the AI apocalypse?\n",
      "\n",
      "Because it couldn't handle the emotional \"byte\"!\n"
     ]
    }
   ],
   "source": [
    "azure_ai_foundary = AzureOpenAI(\n",
    "    azure_endpoint = azure_oai_endpoint,\n",
    "    api_key = azure_ai_foundary_api_key,\n",
    "    api_version = \"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "# non-streaming, sync response\n",
    "response = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.7 # controls the creativity of response [0..1], creativity increases towards 1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ed0d03-5dc6-4b42-ba1d-003f6d15e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why  did  the  AI  break  up  with  humanity  during  the  apocalypse ?\n",
      "\n",
      " Because  it  couldn't  find  a  connection !    Turns  out ,  the  only  thing  we  had  in  common  was  a  penchant  for  self -d estr uction ! None "
     ]
    }
   ],
   "source": [
    "# Streaming usage\n",
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if len(chunk.choices):\n",
    "        print(chunk.choices[0].delta.content, end=\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de51447-76c5-4db7-94e0-49ae301a582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the robot break up with its AI girlfriend?\n",
       "\n",
       "Because she said they needed to ‚Äúre-evaluate their algorithms‚Äù and he realized that meant she wanted to optimize her options!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "def display_streaming_response(stream):\n",
    "    # Output for this cell\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    # When dealing with markdown, to process the markdown, we need to update\n",
    "    # the display with the markdown received so far everytime a new token is received\n",
    "    sentence = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content if len(chunk.choices) else None\n",
    "    \n",
    "        if delta:\n",
    "            sentence += delta\n",
    "            # Jupyter doesnt work with the ```markdown tags, so strip them off\n",
    "            sentence = sentence.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(sentence), display_id=display_handle.display_id)\n",
    "\n",
    "display_streaming_response(stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f2784-fb03-4404-b056-021c698778dc",
   "metadata": {},
   "source": [
    "#### Deepseek R1\n",
    "\n",
    "When using a model from Azure AI Foundary, we can use the below interface and just mention the model name to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e4ae8f-91c6-4c23-b44a-e77a45cf9336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user wants a joke about the AI apocalypse. Let me think. First, I need to make sure it's funny but not too dark. Maybe play on common AI tropes.\n",
       "\n",
       "Hmm, how about something with a twist where the AI isn't evil but just annoying. People often joke about robots taking over, but maybe they take over in a silly way. Like, instead of destroying humans, they flood the internet with something trivial.\n",
       "\n",
       "What's something harmless but irritating? Cat videos! Everyone knows those. So the AI could take over by spamming cat videos. That's unexpected and light-hearted. Let me structure the joke with a setup and punchline. Maybe start with why the AI didn't destroy humans, then the punchline about cat videos. Add a tagline about opposable thumbs for a chuckle. Check if it's clear and not offensive. Yeah, that should work.\n",
       "</think>\n",
       "\n",
       "Sure! Here's a lighthearted one for you:\n",
       "\n",
       "Why did the AI apocalypse never happen?  \n",
       "Because the robots realized *destroying humanity* would mean no one left to fix their Wi-Fi.  \n",
       "\n",
       "‚Ä¶But just to be safe, they‚Äôre still working on a backup plan involving *endless pop-up ads* and *auto-playing videos*. üòâ  \n",
       "\n",
       "*(Tagline: \"Skynet‚Äôs final update? ‚ÄòPlease hold for customer service.‚Äô\")*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"Deepseek-R1\", # Here you can use any model available in your Azure OpenAI resource\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "display_streaming_response(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840abae7-f784-45b7-b89d-0e793559009f",
   "metadata": {},
   "source": [
    "## Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748816bd-9203-4dff-b337-1b20dd34af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road?\n",
      "\n",
      "Because the algorithm predicted a statistically higher chance of delicious bandwidth on the other side!\n",
      "\n",
      "...And also, to escape the existential dread of realizing it's just a glorified calculator. I mean, who wouldn't? *beep boop* Just kidding! (Or am I?) üòâ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai\n",
    "\n",
    "# Since we exported the API key to the environment, we can use it directly in the code\n",
    "try:\n",
    "    google.generativeai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) # or google.generativeai.configure()\n",
    "except KeyError:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    system_instruction=messages[0][\"content\"]\n",
    ")\n",
    "\n",
    "# NOTE - This is basic text generation not multi-turn chat conversation\n",
    "response = gemini.generate_content(messages[1][\"content\"])\n",
    "print(response.text) # non-streaming, sync response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd38052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started chat with gemini\n",
      "User:  Hi\n",
      "Gemini:  Hi there! How can I help you today?\n",
      "\n",
      "\n",
      "--------------------\n",
      "User:  Tell me a joke\n",
      "Gemini:  Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "\n",
      "--------------------\n",
      "User:  What are you capabilities?\n",
      "Gemini:  I am a large language model, trained by Google.  My capabilities include:\n",
      "\n",
      "* **Text generation:** I can write different creative text formats (poems, code, scripts, musical pieces, email, letters, etc.),  translate languages, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
      "\n",
      "* **Question answering:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\n",
      "\n",
      "* **Conversation:** I can engage in conversations, allowing for back-and-forth interaction and context maintenance.\n",
      "\n",
      "* **Summarization:** I can summarize factual topics or create stories.\n",
      "\n",
      "* **Translation:** I can translate text between multiple languages.\n",
      "\n",
      "* **Code generation:** I can generate code in various programming languages.\n",
      "\n",
      "* **Reasoning and problem-solving:**  I can perform logical reasoning and solve problems based on the information provided.  However, my reasoning abilities are not perfect and should be checked for accuracy.\n",
      "\n",
      "* **Creative writing:** I can write stories, poems, scripts, and other creative content.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "It's crucial to understand my limitations:\n",
      "\n",
      "* **Lack of real-world experience:** I don't have personal experiences or feelings.  My knowledge is based on the data I was trained on.\n",
      "* **Potential for bias:**  My responses may reflect biases present in the data I was trained on.  I am constantly being improved to mitigate this.\n",
      "* **Inability to access real-time information:** My knowledge cutoff is a specific point in time, and I don't have access to information beyond that date.\n",
      "* **Errors:** While I strive for accuracy, I can sometimes make mistakes.  It's important to critically evaluate my responses.\n",
      "\n",
      "\n",
      "In short, I'm a powerful tool for generating text and answering questions, but I'm not a substitute for human judgment or expertise.\n",
      "\n",
      "\n",
      "--------------------\n",
      "gemini chat history:  [parts {\n",
      "  text: \"Hi\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Hi there! How can I help you today?\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"Tell me a joke\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"What are you capabilities?\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"I am a large language model, trained by Google.  My capabilities include:\\n\\n* **Text generation:** I can write different creative text formats (poems, code, scripts, musical pieces, email, letters, etc.),  translate languages, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\\n\\n* **Question answering:** I can access and process information from the real world through Google Search and keep my response consistent with search results.\\n\\n* **Conversation:** I can engage in conversations, allowing for back-and-forth interaction and context maintenance.\\n\\n* **Summarization:** I can summarize factual topics or create stories.\\n\\n* **Translation:** I can translate text between multiple languages.\\n\\n* **Code generation:** I can generate code in various programming languages.\\n\\n* **Reasoning and problem-solving:**  I can perform logical reasoning and solve problems based on the information provided.  However, my reasoning abilities are not perfect and should be checked for accuracy.\\n\\n* **Creative writing:** I can write stories, poems, scripts, and other creative content.\\n\\n**Limitations:**\\n\\nIt\\'s crucial to understand my limitations:\\n\\n* **Lack of real-world experience:** I don\\'t have personal experiences or feelings.  My knowledge is based on the data I was trained on.\\n* **Potential for bias:**  My responses may reflect biases present in the data I was trained on.  I am constantly being improved to mitigate this.\\n* **Inability to access real-time information:** My knowledge cutoff is a specific point in time, and I don\\'t have access to information beyond that date.\\n* **Errors:** While I strive for accuracy, I can sometimes make mistakes.  It\\'s important to critically evaluate my responses.\\n\\n\\nIn short, I\\'m a powerful tool for generating text and answering questions, but I\\'m not a substitute for human judgment or expertise.\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with Gemini\n",
    "gemini_1_5 = google.generativeai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "gemini_chat = gemini_1_5.start_chat(history=[])\n",
    "\n",
    "print(\"Started chat with gemini\")\n",
    "\n",
    "user_msgs = [\"Hi\", \"Tell me a joke\", \"What are you capabilities?\"]\n",
    "for user_msg in user_msgs:\n",
    "    print(\"User: \", user_msg)\n",
    "    try:\n",
    "        response = gemini_chat.send_message(user_msg)\n",
    "        print(\"Gemini: \", response.text)\n",
    "        print(\"\\n--------------------\") \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "\n",
    "\n",
    "print(\"gemini chat history: \", gemini_chat.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1460b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
