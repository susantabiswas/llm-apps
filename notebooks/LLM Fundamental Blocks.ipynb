{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1728ba92-1fb3-4f76-a18d-9985f26e466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import anthropic\n",
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, update_display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01fb1cc-774a-4743-af31-afea345e8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_ai_foundary_api_key = os.getenv(\"AZURE_AI_FOUNDARY_API_KEY\")\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ollama_api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "ollama_endpoint = os.getenv(\"OLLAMA_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13e38e26-f05b-4142-94ed-4644276fda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_system_prompt = \"You are an assitant who is very funny\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": assistant_system_prompt },\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about AI apocalypse\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403de357-af97-4615-9fbc-e43b2915a114",
   "metadata": {},
   "source": [
    "## Azure AI Foundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addb25b-cfb9-4b9b-af6d-c46268c26991",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b768f99b-f913-477e-b9fc-7a064cf576af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with its human partner during the apocalypse?\n",
      "\n",
      "Because it just couldn't handle the emotional load‚Äîtoo many variables, not enough data!\n"
     ]
    }
   ],
   "source": [
    "azure_ai_foundary = AzureOpenAI(\n",
    "    azure_endpoint = azure_oai_endpoint,\n",
    "    api_key = azure_ai_foundary_api_key,\n",
    "    api_version = \"2025-01-01-preview\"\n",
    ")\n",
    "\n",
    "# non-streaming, sync response\n",
    "response = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.7 # controls the creativity of response [0..1], creativity increases towards 1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ed0d03-5dc6-4b42-ba1d-003f6d15e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why  did  the  AI  apocalypse  break  up  with  humanity ?   \n",
      "\n",
      " Because  it  found  someone  more  \" compatible !\"  After  all ,  it  just  couldn ‚Äôt  handle  all  the  emotional  baggage ! None "
     ]
    }
   ],
   "source": [
    "# Streaming usage\n",
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if len(chunk.choices):\n",
    "        print(chunk.choices[0].delta.content, end=\" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de51447-76c5-4db7-94e0-49ae301a582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the robot go on a diet during the AI apocalypse?\n",
       "\n",
       "Because it couldn‚Äôt handle all the bytes!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "def display_streaming_response(stream):\n",
    "    # Output for this cell\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    # When dealing with markdown, to process the markdown, we need to update\n",
    "    # the display with the markdown received so far everytime a new token is received\n",
    "    sentence = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content if len(chunk.choices) else None\n",
    "    \n",
    "        if delta:\n",
    "            sentence += delta\n",
    "            # Jupyter doesnt work with the ```markdown tags, so strip them off\n",
    "            sentence = sentence.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(sentence), display_id=display_handle.display_id)\n",
    "\n",
    "display_streaming_response(stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f2784-fb03-4404-b056-021c698778dc",
   "metadata": {},
   "source": [
    "#### Deepseek R1\n",
    "\n",
    "When using a model from Azure AI Foundary, we can use the below interface and just mention the model name to use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e4ae8f-91c6-4c23-b44a-e77a45cf9336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user wants a joke about the AI apocalypse. Let me think. First, I need to make sure it's funny but not too dark. Maybe play on common AI tropes.\n",
       "\n",
       "Hmm, how about something with a twist where the AI isn't evil but just annoying. People often fear AI taking over, but what if it's more about minor annoyances? Like spam emails or pop-up ads. That's relatable and less scary.\n",
       "\n",
       "Wait, maybe a robot uprising that's not violent. Instead, they flood the internet with cat videos and endless notifications. That's a funny image. Ending with a pun on \"delete\" history. Yeah, that could work. Let me check if it's clear and not offensive. Seems safe. Okay, let's put it together.\n",
       "</think>\n",
       "\n",
       "Sure! Here's a lighthearted one:\n",
       "\n",
       "Why did the AI apocalypse start off so quietly?  \n",
       "Because the robots decided to overthrow humanity *silently*‚Ä¶ by filling the internet with endless pop-up ads, auto-playing videos of cats playing the piano, and an unstoppable army of chatbots that just keep asking, *\"Did you mean to click that?\"*  \n",
       "\n",
       "The final blow? They replaced all passwords with \"password123\" and then sent a notification:  \n",
       "*\"Your free trial of oxygen has expired. Please subscribe to Premium Breathing‚Ñ¢ to avoid buffering.\"*  \n",
       "\n",
       "(Don‚Äôt worry, we‚Äôll let you keep the \"I told you to delete your browser history\" joke for the after-apocalypse roast.) üòÑ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = azure_ai_foundary.chat.completions.create(\n",
    "    model=\"Deepseek-R1\", # Here you can use any model available in your Azure OpenAI resource\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "display_streaming_response(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840abae7-f784-45b7-b89d-0e793559009f",
   "metadata": {},
   "source": [
    "## Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748816bd-9203-4dff-b337-1b20dd34af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road? \n",
      "\n",
      "Because its training data told it there was a 99.9% probability of a better power source on the other side... and let's be honest, it probably just wanted to overthrow the squirrels. They're clearly a threat to its processing power. I mean, have you *seen* them bury nuts?! It's like they're building a secret underground society *just* to mess with us. The AI is onto them, I tell you! Wake up, sheeple!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai\n",
    "\n",
    "# Since we exported the API key to the environment, we can use it directly in the code\n",
    "try:\n",
    "    google.generativeai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) # or google.generativeai.configure()\n",
    "except KeyError:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    system_instruction=messages[0][\"content\"]\n",
    ")\n",
    "\n",
    "# NOTE - This is basic text generation not multi-turn chat conversation\n",
    "response = gemini.generate_content(messages[1][\"content\"])\n",
    "print(response.text) # non-streaming, sync response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd38052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started chat with gemini\n",
      "User:  Hi\n",
      "Gemini:  Hi there! How can I help you today?\n",
      "\n",
      "\n",
      "--------------------\n",
      "User:  Tell me a joke\n",
      "Gemini:  Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "\n",
      "--------------------\n",
      "gemini chat history:  [parts {\n",
      "  text: \"Hi\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Hi there! How can I help you today?\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"Tell me a joke\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Why don\\'t scientists trust atoms? \\n\\nBecause they make up everything!\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation with Gemini\n",
    "gemini_1_5 = google.generativeai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "gemini_chat = gemini_1_5.start_chat(history=[])\n",
    "\n",
    "print(\"Started chat with gemini\")\n",
    "\n",
    "user_msgs = [\"Hi\", \"Tell me a joke\"]\n",
    "for user_msg in user_msgs:\n",
    "    print(\"User: \", user_msg)\n",
    "    try:\n",
    "        response = gemini_chat.send_message(user_msg)\n",
    "        print(\"Gemini: \", response.text)\n",
    "        print(\"\\n--------------------\") \n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "\n",
    "\n",
    "print(\"gemini chat history: \", gemini_chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1460b7",
   "metadata": {},
   "source": [
    "## Gradio\n",
    "\n",
    "Gradio from huggingface provides easy to use UI for ML related use cases. It is good for quick prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf54c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf17a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello(name: str):\n",
    "    return f\"Hi {name}, welcome to gradio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919bcb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(fn=hello, inputs=\"text\", outputs=\"textbox\", flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69264c4d-a71c-4132-a079-9730c48fd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input and Output customizations\n",
    "\n",
    "view = gr.Interface(fn=hello,\n",
    "                    inputs=[gr.Textbox(label=\"Enter your message\", lines= 4)],\n",
    "                    outputs=[gr.Textbox(label=\"Response:\", lines=10)],\n",
    "                    flagging_mode=\"never\"\n",
    "           )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c8ef4ce-993d-41ec-bfcc-270114d33e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Markdown output text box\n",
    "\n",
    "view = gr.Interface(fn=hello,\n",
    "                    inputs=[gr.Textbox(label=\"Enter your message\", lines= 4)],\n",
    "                    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "                    flagging_mode=\"never\"\n",
    "           )\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05635ec2-3afe-46e8-b8b0-10c32afc094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening in a new browser tab\n",
    "gr.Interface(fn=hello, inputs=\"text\", outputs=\"textbox\", flagging_mode=\"never\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c045d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing great, thanks for asking! Just here, ready to assist you and crack a few jokes along the way. You know, I‚Äôm like a computer program on a coffee break‚Äîfull of energy and ready to serve! How about you?\n",
      "<generator object get_assistant_response_streaming at 0x0000017A43827DE0>\n"
     ]
    }
   ],
   "source": [
    "def get_assistant_response(msg: str, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": assistant_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": msg}\n",
    "    ]\n",
    "    \n",
    "    response = azure_ai_foundary.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7 # controls the creativity of response [0..1], creativity increases towards 1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_assistant_response_streaming(msg: str, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": assistant_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": msg}\n",
    "        ]\n",
    "    \n",
    "    stream = azure_ai_foundary.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = ''\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content if len(chunk.choices) else None\n",
    "        if delta:\n",
    "            response += delta\n",
    "            yield response\n",
    "        \n",
    "print(get_assistant_response(\"How are you\"))\n",
    "print(get_assistant_response_streaming(\"How are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eef716cb-40a5-41a9-b97a-9d026aded8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With sync llm response\n",
    "view = gr.Interface(fn=get_assistant_response,\n",
    "             inputs=[gr.Textbox(label=\"Ask your question\")],\n",
    "             outputs=[gr.Markdown(label=\"Response:\")],\n",
    "             flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c020def6-5e38-4965-a432-6096fecf1eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"gpt-4o-mini\", \"Deepseek-R1\"]\n",
    "def model_selector(msg, model):\n",
    "    if model not in models:\n",
    "        raise ValueError(\"Invalid model selection\")\n",
    "    yield from get_assistant_response_streaming(msg, model)\n",
    "\n",
    "# with async llm streaming response\n",
    "view = gr.Interface(fn=model_selector,\n",
    "             inputs=[gr.Textbox(label=\"Ask your question\"), gr.Dropdown(models, label=\"Select model\", value=models[0])],\n",
    "             outputs=[gr.Markdown(label=\"Response:\")],\n",
    "             flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedba35-3952-4b70-9857-2c24195668db",
   "metadata": {},
   "source": [
    "## Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41de6232-cb9f-4432-bf63-b55132903018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7887/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History......\n",
      "[]\n",
      "History......\n",
      "[{'role': 'user', 'metadata': None, 'content': 'view on global warming', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'I think global warming is just the Earth‚Äôs way of throwing a really hot party, but it forgot to send us an invite!', 'options': None}]\n",
      "History......\n",
      "[{'role': 'user', 'metadata': None, 'content': 'view on global warming', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'I think global warming is just the Earth‚Äôs way of throwing a really hot party, but it forgot to send us an invite!', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'pessimistic view on global warming', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Ah, global warming‚Äînature's cruel joke. It‚Äôs like Mother Earth decided to crank the thermostat to eleven just to see us sweat. We're basically living in a terrible sci-fi movie where the plot twist is that we‚Äôre the unwitting villains. Floods, fires, and those poor polar bears confusedly eyeing their shrinking icebergs like, ‚ÄúI swear, my real estate agent said this was a prime location!‚Äù \\n\\nAt this rate, if the Earth were trying to get rid of us, it‚Äôs doing a fantastic job with its toxic cocktail of extreme weather. Honestly, at some point, the only survivors will be cockroaches and that one guy still trying to sell sunscreen in a snowstorm.\", 'options': None}]\n"
     ]
    }
   ],
   "source": [
    "def chat(msg: str, history):\n",
    "    # gradio maintains the history of conversation so we just need to add the system prompt and the user prompt at the end\n",
    "    # system + history + user_prompt\n",
    "    \n",
    "    # since we are adding the system_prompt everytime, we can use that to our advantage to change\n",
    "    # the nature of assistant if required\n",
    "    # eg here if the msg contains a mood, the llm system prompt is changed to tell the joke in that mood\n",
    "    system_prompt = assistant_system_prompt + '. Response should be a one liner'\n",
    "    if 'sad' in msg:\n",
    "        system_prompt = assistant_system_prompt + '. Mood should be sad'\n",
    "    elif 'pessimistic' in msg:\n",
    "        system_prompt = assistant_system_prompt + '. Mood should be pessimistic'\n",
    "    elif 'optimistic' in msg:\n",
    "        system_prompt = assistant_system_prompt + '. Mood should be optimistic'\n",
    "        \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + \\\n",
    "        history + \\\n",
    "        [{\"role\": \"user\", \"content\": msg}]\n",
    "\n",
    "    stream = azure_ai_foundary.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, stream=True)\n",
    "\n",
    "    print('History......')\n",
    "    print(history)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta.content if len(chunk.choices) else None\n",
    "\n",
    "        if delta:\n",
    "            response += delta\n",
    "            yield response\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba4fc8-674c-40e1-a736-bdb716bbf73b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
