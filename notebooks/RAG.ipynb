{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1728ba92-1fb3-4f76-a18d-9985f26e466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import anthropic\n",
    "import ollama\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, update_display, Markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01fb1cc-774a-4743-af31-afea345e8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_ai_foundary_api_key = os.getenv(\"AZURE_AI_FOUNDARY_API_KEY\")\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OAI_ENDPOINT\")\n",
    "\n",
    "azure_embed_api_key = os.getenv(\"AZURE_EMBED_KEY\")\n",
    "azure_embed_endpoint = os.getenv(\"AZURE_EMBED_ENDPOINT\")\n",
    "\n",
    "azure_audio_api_key = os.getenv(\"AZURE_AUDIO_API_KEY\")\n",
    "azure_audio_endpoint = os.getenv(\"AZURE_AUDIO_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b768f99b-f913-477e-b9fc-7a064cf576af",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_foundary = AzureOpenAI(\n",
    "    azure_endpoint = azure_oai_endpoint,\n",
    "    api_key = azure_ai_foundary_api_key,\n",
    "    api_version = \"2025-01-01-preview\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04599a0e-8be1-42e3-be9f-d8f77a528e3e",
   "metadata": {},
   "source": [
    "## Langchain RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded2a1ea-d217-4b8e-8ca2-bb52ad9df903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# vector embeddings for vector db\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7d284-fb36-49ad-be1e-3b3deadfa3be",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Load all the documents which can be vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767fbef6-9105-4ce2-909d-0356bcd1f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder data\\business, doc_type: business\n",
      "Found 5 docs of doc_type: business\n",
      "Folder data\\developers, doc_type: developers\n",
      "Found 4 docs of doc_type: developers\n",
      "Folder data\\projects, doc_type: projects\n",
      "Found 5 docs of doc_type: projects\n",
      "Total docs: 14\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('data', '*')\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "folders = glob.glob(data_path)\n",
    "\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "\n",
    "    print(f'Folder {folder}, doc_type: {doc_type}')\n",
    "    loader = DirectoryLoader(folder,\n",
    "                glob=\"**/*.md\", \n",
    "                loader_cls=TextLoader,\n",
    "                loader_kwargs=text_loader_kwargs)\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(f'Found {len(docs)} docs of doc_type: {doc_type}') \n",
    "    for doc in docs:\n",
    "        doc.metadata['doc_type'] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f'Total docs: {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e30bca-d934-4b02-b945-f85ce2e4c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of documents\n",
      "{'source': 'data\\\\business\\\\cloudsync_impact.md', 'doc_type': 'business'}\n",
      "{'source': 'data\\\\business\\\\dataviz_impact.md', 'doc_type': 'business'}\n",
      "{'source': 'data\\\\business\\\\fraud_detection_impact.md', 'doc_type': 'business'}\n",
      "{'source': 'data\\\\business\\\\healthtrack_impact.md', 'doc_type': 'business'}\n",
      "{'source': 'data\\\\business\\\\microservices_impact.md', 'doc_type': 'business'}\n",
      "{'source': 'data\\\\developers\\\\alex_chen.md', 'doc_type': 'developers'}\n",
      "{'source': 'data\\\\developers\\\\john_smith.md', 'doc_type': 'developers'}\n",
      "{'source': 'data\\\\developers\\\\raj_patel.md', 'doc_type': 'developers'}\n",
      "{'source': 'data\\\\developers\\\\sarah_johnson.md', 'doc_type': 'developers'}\n",
      "{'source': 'data\\\\projects\\\\cloudsync.md', 'doc_type': 'projects'}\n",
      "{'source': 'data\\\\projects\\\\dataviz.md', 'doc_type': 'projects'}\n",
      "{'source': 'data\\\\projects\\\\fraud_detection.md', 'doc_type': 'projects'}\n",
      "{'source': 'data\\\\projects\\\\healthtrack.md', 'doc_type': 'projects'}\n",
      "{'source': 'data\\\\projects\\\\microservices.md', 'doc_type': 'projects'}\n"
     ]
    }
   ],
   "source": [
    "# split the documents into chunks, the chunk size and amount of overlaps between the chunks can have\n",
    "# impact on the results\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap= 200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print('List of documents')\n",
    "for chunk in chunks:\n",
    "    print(chunk.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ae560",
   "metadata": {},
   "source": [
    "### Direct Azure OpenAI usage for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c179b1-bacf-4250-b880-9aaa8231680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=3072, [0.022330209612846375, -0.002088305074721575, ..., -0.014379994943737984, 0.006100048776715994]\n",
      "data[1]: length=3072, [0.011640272103250027, 0.005252661183476448, ..., -0.028720801696181297, -0.0025770869106054306]\n",
      "data[2]: length=3072, [0.016326788812875748, -0.0018455119570717216, ..., -0.005349587649106979, 0.006049444433301687]\n",
      "Usage(prompt_tokens=6, total_tokens=6)\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "embed_model = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=azure_embed_endpoint,\n",
    "    api_key=azure_embed_api_key\n",
    ")\n",
    "\n",
    "deployment = \"text-embedding-3-large\"\n",
    "response = embed_model.embeddings.create(\n",
    "    input=[\"first phrase\",\"second phrase\",\"third phrase\"],\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "for item in response.data:\n",
    "    length = len(item.embedding)\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13b923",
   "metadata": {},
   "source": [
    "### Embeddings using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ab94ba-79fd-47a7-8f57-8c19a4d719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm(env_file: str = None):\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_endpoint=azure_oai_endpoint,\n",
    "        azure_deployment=\"gpt-4o-mini\",\n",
    "        openai_api_version=\"2025-01-01-preview\",\n",
    "    )\n",
    "\n",
    "    embed_model_name = \"text-embedding-3-large\"\n",
    "    embed_api_version = \"2024-12-01-preview\" #\"2024-02-01\"\n",
    "    azure_embed_endpoint = os.environ['AZURE_EMBED_ENDPOINT']\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model=embed_model_name,\n",
    "        azure_deployment=embed_model_name,\n",
    "        azure_endpoint=azure_embed_endpoint,\n",
    "        openai_api_version=embed_api_version,\n",
    "        api_key=azure_embed_api_key)\n",
    "    \n",
    "    return llm, embeddings\n",
    "\n",
    "llm, azure_embeddings = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f548f1-2cee-425d-a0f8-1b79a9caafae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma db path notebooks\\vector_store\\chroma_rag_db\n",
      "db cleared\n",
      "Vectorstore created with 14 documents\n"
     ]
    }
   ],
   "source": [
    "CHROMA_DB = os.path.join(\"notebooks\", \"vector_store\", \"chroma_rag_db\")\n",
    "print(\"Chroma db path\", CHROMA_DB)\n",
    "      \n",
    "# clear the db\n",
    "if os.path.exists(CHROMA_DB):\n",
    "    Chroma(persist_directory=CHROMA_DB, embedding_function=azure_embeddings).delete_collection()\n",
    "    print('db cleared')\n",
    "\n",
    "# vectorize the docs\n",
    "vectorstore = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=azure_embeddings,\n",
    "                                    persist_directory=CHROMA_DB)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27655178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['0233e4dd-4ab5-4fcd-b616-9ec75396a7ce'], 'embeddings': array([[ 0.00643417, -0.01768304, -0.01689518, ..., -0.01734747,\n",
      "        -0.00097479, -0.01140935]], shape=(1, 3072)), 'documents': None, 'uris': None, 'data': None, 'metadatas': None, 'included': [<IncludeEnum.embeddings: 'embeddings'>]}\n",
      "Dimensions:  3072\n"
     ]
    }
   ],
   "source": [
    "# Inspect the vectorstore\n",
    "collection = vectorstore._collection\n",
    "sample_vector_embed = collection.get(limit=1, include=['embeddings'])\n",
    "print(sample_vector_embed)\n",
    "print('Dimensions: ', len(sample_vector_embed['embeddings'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3a989",
   "metadata": {},
   "source": [
    "### Visualizing Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530f12b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
