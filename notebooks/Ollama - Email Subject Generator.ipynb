{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f84f58a-0b71-4160-9dd0-680afe3a2c21",
   "metadata": {},
   "source": [
    "# Email Subject Generator\n",
    "\n",
    "We will use a LLM to generate an appropriate Email subject for a given email body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5baceb-e301-4dbe-bf14-25416835bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbff794d-ce0e-40bb-9a08-b4ad5afffc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM details\n",
    "REST_OLLAMA_ENDPOINT = \"http://localhost:11434/api/chat\"\n",
    "MODEL = \"deepseek-r1:1.5b\" # Thinking model, the thoughts are enclosed in <think> tags\n",
    "OLLAMA_KEY = \"ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cb5ea-64ae-4d64-95ea-6a75cc9fd032",
   "metadata": {},
   "source": [
    "### Using REST Endpoint with local Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c49f8141-30db-4cd5-bbf3-87ef67c3c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the ollama package, we can also use the REST endpoints to do the operations.\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hi, how are you?\"}]\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": messages,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(REST_OLLAMA_ENDPOINT,\n",
    "                         json=payload,\n",
    "                         headers={\"Content-Type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003818d0-67d2-48f6-8137-f5aecb18217f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'deepseek-r1:1.5b',\n",
       " 'created_at': '2025-04-01T07:13:31.1837912Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"<think>\\n\\n</think>\\n\\nHello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today? ðŸ˜Š\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 589407300,\n",
       " 'load_duration': 29056300,\n",
       " 'prompt_eval_count': 9,\n",
       " 'prompt_eval_duration': 3632500,\n",
       " 'eval_count': 44,\n",
       " 'eval_duration': 556718500}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b5155-5a53-4bbb-acf0-ad98eacfc8c9",
   "metadata": {},
   "source": [
    "### Using Ollama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba948b10-4ff6-41f0-9133-f6c2b157fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Using Ollama lib\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa909f1e-2493-44fc-a927-a3165eb573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hi! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How are *you* doing? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# We can also use the openAI lib by just setting the endpoint of the local ollama server\n",
    "from openai import OpenAI\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hi, how are you?\"}]\n",
    "\n",
    "# Since ollama follows similar API path like OpenAI, hence the lib tries to read the endpoint - http://localhost:11434/v1/chat/completions\n",
    "ollama_server = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "response = ollama_server.chat.completions.create(model=MODEL, messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "906d2e55-9a57-46c0-bfc4-e77ef97b9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt():\n",
    "    return \"\"\"You are an expert writer. You will be given contents of an email and your task is to suggest the best possible subject for it.\n",
    "    Try to create a subject which increases the likelihood of the reader reading it by reading the subject. The output should just be the subject line.\n",
    "    \"\"\"\n",
    "\n",
    "def user_prompt(content: str):\n",
    "    return f\"\"\"Here is an email body, suggest a subject for it. Email is as follows:\n",
    "    {content}\n",
    "    \"\"\"\n",
    "\n",
    "def create_messages(content: str):\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": user_prompt(content)}]\n",
    "\n",
    "def suggest_email_subject(email: str):\n",
    "    email_messages = create_messages(email)\n",
    "    reponse = ollama_server.chat.completions.create(model=\"llama3.2:latest\", messages=email_messages)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc3a2046-ffa9-4ec9-a01e-ca774ac129d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the email body, a suitable subject could be:\n",
      "\n",
      "\"Job Opportunity Inquiry for Engineering Roles\"\n",
      "\n",
      "This subject accurately conveys the purpose of the email and highlights your interest in potential job openings.\n",
      "\n",
      "Alternatively, you could also consider these other options:\n",
      "\n",
      "* \"Seeking Engineering Opportunities\"\n",
      "* \"Request for Engineering Job Openings\"\n",
      "* \"Career Update: Experiencing Increased YOE - Seeking New Opportunities\"\n",
      "* \"Exploring Engineering Positions at XYZ\"\n",
      "\n",
      "Choose the one that best fits your tone and style!\n"
     ]
    }
   ],
   "source": [
    "email = \"\"\"Hi <recepient>, I am currently working at XYZ and have 5+ YOE. I am looking for some engineering roles, please let me know if you have any open positions. \n",
    "Tech Stack - Python, C#, Kubernetes, MySQL, Distributed Systems, Microservices\"\"\"\n",
    "\n",
    "subject = suggest_email_subject(email)\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48cffce4-053a-428a-9986-2ff4dbde3db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the email body, a suitable subject could be:\n",
       "\n",
       "\"Job Opportunity Inquiry for Engineering Roles\"\n",
       "\n",
       "This subject accurately conveys the purpose of the email and highlights your interest in potential job openings.\n",
       "\n",
       "Alternatively, you could also consider these other options:\n",
       "\n",
       "* \"Seeking Engineering Opportunities\"\n",
       "* \"Request for Engineering Job Openings\"\n",
       "* \"Career Update: Experiencing Increased YOE - Seeking New Opportunities\"\n",
       "* \"Exploring Engineering Positions at XYZ\"\n",
       "\n",
       "Choose the one that best fits your tone and style!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af606a67-41ce-42b0-b3ca-927b343bdcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
