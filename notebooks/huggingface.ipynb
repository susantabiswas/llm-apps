{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e117e122",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "Different samples for using LLMs from Huggingface.\n",
    "Huggingface provides pipelines for doing common ML use cases like sentiment analysis, classification etc.\n",
    "For for complex operations, it also provides more fine tuned controls for dealing with the models.\n",
    "\n",
    "https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Fpipelines\n",
    "\n",
    "https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fdocs%2Fdiffusers%2Fen%2Fapi%2Fpipelines%2Foverview\n",
    "\n",
    "Tokenizers Colab\n",
    "https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing\n",
    "\n",
    "Model Loading Colab\n",
    "https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing#scrollTo=hhOgL1p_T6-b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa6728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SeekerG\\Desktop\\Dev\\github\\llm-apps\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b04b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.7.0+cu128\n",
      "Using device: cuda\n",
      "GPU count 1\n",
      "GPU name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 1.1 GB\n",
      "Cached:    1.2 GB\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)\n",
    "print('GPU count', torch.cuda.device_count())\n",
    "if device.type == 'cuda':\n",
    "    print('GPU name:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5d461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Before we can use the different huggingface pipelines, we need to login to the huggingface hub.\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN is None:\n",
    "    raise ValueError(\"Please set the HF_TOKEN environment variable.\")\n",
    "\n",
    "login(HF_TOKEN, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53076003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998794794082642}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "# To use GPU, pass 'device=True' in the pipeline function.\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"Today is a beautiful day.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873ac7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9995421767234802, 'start': 0, 'end': 6, 'answer': 'Google'}\n"
     ]
    }
   ],
   "source": [
    "# Question Answering with Context\n",
    "\n",
    "qa = pipeline(\"question-answering\")\n",
    "result = qa(question=\"Which is the best search tool?\", context=\"Google is believed to be the best search tool but other players are catching up.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a6ae442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "  Stable represents the most currently tested and supported version of PyTorch . Preview is available if you want the latest, not fully tested, builds that are generated nightly . Note that LibTorch is only available for C++ .\n"
     ]
    }
   ],
   "source": [
    "# Text Summarization\n",
    "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
    "text = \"\"\"Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy), depending on your package manager. You can also install previous versions of PyTorch. Note that LibTorch is only available for C++. \n",
    "\"\"\"\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print('Summary\\n', summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d05849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\SeekerG\\Desktop\\Dev\\github\\llm-apps\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SeekerG\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': \"Hugging Face's Transformers library is amazing!\", 'labels': ['technology', 'sports', 'politics'], 'scores': [0.9493840336799622, 0.032250143587589264, 0.01836583949625492]}\n"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
    "result = classifier(\"Hugging Face's Transformers library is amazing!\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd95f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\SeekerG\\Desktop\\Dev\\github\\llm-apps\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SeekerG\\.cache\\huggingface\\hub\\models--google-t5--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les Data Scientists ont été vraiment étonnés par la puissance et la simplicité de l'API du pipeline HuggingFace.\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
    "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030d4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
